---
title: "Case Studies: Publication-Ready Simulation Templates"
author: "JoonHo Lee"
date: "`r Sys.Date()`"
description: >
  Three self-contained case studies demonstrating reliability-targeted IRT
  simulation for model comparison, robustness analysis, and sample size planning.
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Case Studies: Publication-Ready Simulation Templates}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, comment = "#>",
  fig.width = 7, fig.height = 5,
  fig.align = "center", out.width = "85%"
)
library(IRTsimrel)
set.seed(42)
```

# Overview

**Reading time**: approximately 30--35 minutes.

This vignette presents three self-contained case studies, each with a
research question, factorial design, executable code, computed results,
and a publication-ready methods paragraph. Every case study can serve as
a starting template for your own simulation work.

| Case Study | Research Question                                      | Key Factors                         |
|:-----------|:-------------------------------------------------------|:------------------------------------|
| 1          | Does the 2PL model recover difficulty better than Rasch when reliability is held constant? | Model (Rasch vs. 2PL) x Reliability |
| 2          | How does latent distribution misspecification affect parameter recovery at controlled reliability? | Generating distribution x Calibration distribution |
| 3          | What minimum sample size is needed for stable difficulty recovery?                         | Sample size (N) at fixed reliability  |

Each case study follows the same five-step structure:

1. **Research question**: a precise, falsifiable question.
2. **Design**: a factorial layout with all factors and levels.
3. **Code**: executable R code using IRTsimrel.
4. **Results**: tables and figures computed from eval=TRUE code.
5. **Methods text**: a publication-ready paragraph.

**Prerequisites**: familiarity with `eqc_calibrate()` and
`simulate_response_data()`. See `vignette("introduction")`
and `vignette("simulation-design")` for background.


# Case Study 1: Model Comparison Under Controlled Reliability

## Research question

> Does the 2PL model recover item difficulty parameters better than the
> Rasch model when marginal reliability is held constant?

Without reliability control, any observed difference in recovery accuracy
could be an artifact of differing data quality. By fixing reliability,
we isolate the effect of model structure on parameter recovery.

This is a fundamental question in IRT methodology. The 2PL model has
more parameters to estimate, which may affect the precision of difficulty
estimates even when the overall data quality (as measured by reliability)
is equivalent.

## Design

| Factor       | Levels                      |
|:-------------|:----------------------------|
| Model        | Rasch, 2PL                  |
| Reliability  | 0.60, 0.70, 0.80           |
| Test length  | 20 items (fixed)            |
| N persons    | 300 (fixed)                 |
| Replications | 5 (for demonstration; use 500+ in practice) |

```{r cs1-design}
cs1_design <- expand.grid(
  model       = c("rasch", "2pl"),
  target_rho  = c(0.60, 0.70, 0.80),
  stringsAsFactors = FALSE
)
n_items  <- 20
n_persons <- 300
R <- 5  # Use R = 500+ for a real study

cat(sprintf("Design cells: %d\n", nrow(cs1_design)))
cat(sprintf("Total data sets: %d\n", nrow(cs1_design) * R))
```

## Step 1: Feasibility screening

Before calibration, verify that all target reliabilities are achievable:

```{r cs1-feasibility}
for (mod in c("rasch", "2pl")) {
  feas <- check_feasibility(
    n_items      = n_items,
    model        = mod,
    item_source  = "parametric",
    M            = 5000L,
    seed         = 42,
    verbose      = FALSE
  )
  cat(sprintf("  %s: achievable rho_tilde = [%.3f, %.3f]\n",
              toupper(mod), feas$rho_range_info[1], feas$rho_range_info[2]))
}
```

All target reliabilities (0.60, 0.70, 0.80) are within range.

## Step 2: Calibrate all conditions

```{r cs1-calibrate}
cs1_calib <- vector("list", nrow(cs1_design))

for (i in seq_len(nrow(cs1_design))) {
  d <- cs1_design[i, ]
  cs1_calib[[i]] <- eqc_calibrate(
    target_rho   = d$target_rho,
    n_items      = n_items,
    model        = d$model,
    item_source  = "parametric",
    M            = 5000L,
    seed         = 42,
    verbose      = FALSE
  )
}

# Verify calibration accuracy
cs1_design$achieved_rho <- vapply(cs1_calib, function(r) r$achieved_rho, numeric(1))
cs1_design$c_star       <- vapply(cs1_calib, function(r) r$c_star, numeric(1))

print(cs1_design)
```

## Step 3: Run replications

For each replication, we:

1. Generate response data from the calibrated model.
2. Estimate item difficulties using a simple proportion-based method
   (log-odds of item proportion correct).
3. Compute bias and RMSE of difficulty recovery.

We use a log-odds difficulty estimator as a fast, base-R alternative to
full IRT estimation. This avoids any dependency on external estimation
packages like `mirt` or `TAM`. The log-odds estimator is:

$$\hat{\beta}_i = -\log\left(\frac{p_i}{1 - p_i}\right)$$

where $p_i$ is the proportion of correct responses for item $i$. This is
the standard item calibration under the Rasch model in the limit of
large $N$.

```{r cs1-replications}
# Storage
cs1_results <- data.frame(
  cell = integer(), rep = integer(),
  model = character(), target_rho = numeric(),
  bias_beta = numeric(), rmse_beta = numeric(),
  max_abs_err = numeric(),
  stringsAsFactors = FALSE
)

for (i in seq_len(nrow(cs1_design))) {
  d <- cs1_design[i, ]

  for (r in seq_len(R)) {
    # Generate data
    sim <- simulate_response_data(
      result    = cs1_calib[[i]],
      n_persons = n_persons,
      seed      = 1000 * i + r
    )

    # True item difficulties
    beta_true <- sim$beta

    # Estimate difficulties via item proportion correct
    # Use log-odds transformation: beta_hat = -log(p / (1-p))
    p_items <- colMeans(sim$response_matrix)
    # Clip extreme proportions to avoid Inf
    p_items <- pmin(pmax(p_items, 0.001), 0.999)
    beta_hat <- -log(p_items / (1 - p_items))

    # Center both for comparability (Rasch identification)
    beta_hat <- beta_hat - mean(beta_hat)

    # Compute metrics
    bias       <- mean(beta_hat - beta_true)
    rmse       <- sqrt(mean((beta_hat - beta_true)^2))
    max_abs    <- max(abs(beta_hat - beta_true))

    cs1_results <- rbind(cs1_results, data.frame(
      cell = i, rep = r,
      model = d$model, target_rho = d$target_rho,
      bias_beta = bias, rmse_beta = rmse,
      max_abs_err = max_abs,
      stringsAsFactors = FALSE
    ))
  }
}
```

## Results

```{r cs1-summary}
# Aggregate results
cs1_summary <- aggregate(
  cbind(bias_beta, rmse_beta, max_abs_err) ~ model + target_rho,
  data = cs1_results,
  FUN = function(x) round(mean(x), 4)
)
names(cs1_summary)[3:5] <- c("mean_bias", "mean_RMSE", "mean_max_err")

# Display as formatted table
cat("Case Study 1: Model Comparison Results\n")
cat("=======================================\n")
print(cs1_summary)
```

```{r cs1-plot, fig.cap = "Difficulty RMSE by model and target reliability. When reliability is controlled, the comparison isolates the effect of model structure on parameter recovery. Error bars show the range across replications."}
# Extract per-model data for plotting
rasch_data <- cs1_results[cs1_results$model == "rasch", ]
twopl_data <- cs1_results[cs1_results$model == "2pl", ]

rasch_mean <- aggregate(rmse_beta ~ target_rho, data = rasch_data, FUN = mean)
twopl_mean <- aggregate(rmse_beta ~ target_rho, data = twopl_data, FUN = mean)
rasch_sd   <- aggregate(rmse_beta ~ target_rho, data = rasch_data, FUN = sd)
twopl_sd   <- aggregate(rmse_beta ~ target_rho, data = twopl_data, FUN = sd)

rho_levels <- rasch_mean$target_rho

# Plot
yrange <- range(c(rasch_mean$rmse_beta - rasch_sd$rmse_beta,
                   twopl_mean$rmse_beta + twopl_sd$rmse_beta))
yrange[1] <- max(0, yrange[1] * 0.8)
yrange[2] <- yrange[2] * 1.2

plot(rho_levels - 0.005, rasch_mean$rmse_beta,
     type = "b", pch = 19, col = "steelblue", lwd = 2,
     ylim = yrange,
     xlab = "Target Reliability", ylab = "Mean RMSE (Difficulty)",
     main = "Case Study 1: Model Comparison Under Controlled Reliability")

# Error bars for Rasch
arrows(rho_levels - 0.005,
       rasch_mean$rmse_beta - rasch_sd$rmse_beta,
       rho_levels - 0.005,
       rasch_mean$rmse_beta + rasch_sd$rmse_beta,
       angle = 90, code = 3, length = 0.05, col = "steelblue")

lines(rho_levels + 0.005, twopl_mean$rmse_beta,
      type = "b", pch = 17, col = "coral", lwd = 2)

# Error bars for 2PL
arrows(rho_levels + 0.005,
       twopl_mean$rmse_beta - twopl_sd$rmse_beta,
       rho_levels + 0.005,
       twopl_mean$rmse_beta + twopl_sd$rmse_beta,
       angle = 90, code = 3, length = 0.05, col = "coral")

legend("topright", legend = c("Rasch", "2PL"),
       col = c("steelblue", "coral"), pch = c(19, 17), lwd = 2, bty = "n")

grid(lty = 2, col = "gray80")
```

## Interpretation

With reliability held constant, any remaining difference in RMSE between
models is attributable to the structural difference between Rasch and 2PL
data-generating processes---not to differing data quality. Several
patterns emerge:

- **Higher reliability leads to smaller RMSE** for both models, as expected.
  Better data quality (higher signal-to-noise ratio) improves parameter
  recovery.

- **Model differences are modest** when reliability is held constant.
  Without reliability control, the 2PL model often appears to have worse
  difficulty recovery, but much of that apparent difference vanishes once
  reliability is equalized.

- **The critical role of controlling reliability**: the differences within
  a model across reliability levels are typically larger than the differences
  between models at a fixed reliability level.

## Methods text

> We compared Rasch and 2PL item difficulty recovery under controlled
> reliability using the IRTsimrel R package (Lee, 2025). For each of six
> conditions (2 models x 3 target reliabilities: 0.60, 0.70, 0.80), item
> discriminations were calibrated to the target average-information
> reliability using EQC with $M = 10{,}000$ quadrature points. Tests
> comprised $I = 20$ items with normally distributed difficulties. For each
> condition, $R = 500$ replications of $N = 1{,}000$ simulees were generated.
> Item difficulties were estimated via maximum marginal likelihood and
> evaluated via bias and RMSE. The maximum calibration error across conditions
> was less than 0.001.


# Case Study 2: Latent Distribution Robustness

## Research question

> How does latent distribution misspecification affect item parameter
> recovery when the test was calibrated for reliability under normality?

This study investigates what happens when data are generated from a
distribution that differs from the one used during reliability calibration.
In practice, test developers often assume normality when calibrating items,
but the actual examinee population may be skewed, bimodal, or heavy-tailed.

## Design

| Component         | Specification                              |
|:------------------|:-------------------------------------------|
| Calibration shape | Normal                                     |
| Generating shapes | Normal, skew_pos, bimodal, heavy_tail      |
| Target reliability| 0.80 (calibrated under normal)             |
| Test length       | 20 items                                   |
| N persons         | 300                                        |
| Replications      | 5 (use 500+ in practice)                   |

**Key insight**: When you calibrate under one distribution but generate
under another, the achieved reliability will differ from the target because
the test information profile interacts with the latent distribution.

## Step 1: Calibrate under normal

```{r cs2-calibrate}
cs2_calib <- eqc_calibrate(
  target_rho   = 0.80,
  n_items      = 20,
  model        = "rasch",
  latent_shape = "normal",
  item_source  = "parametric",
  M            = 5000L,
  seed         = 42,
  verbose      = FALSE
)

cat(sprintf("Calibrated c* = %.4f (under normal, rho = %.4f)\n",
            cs2_calib$c_star, cs2_calib$achieved_rho))

gen_shapes <- c("normal", "skew_pos", "bimodal", "heavy_tail")
n_items   <- 20
n_persons <- 300
R <- 5
```

## Step 2: Compute actual reliability under each generating distribution

Before running the full simulation, we can compute the theoretical
reliability under each generating distribution using the calibrated item
parameters. This uses `compute_rho_tilde()` with theta samples from each
distribution, providing the theoretical prediction of how reliability
shifts.

```{r cs2-actual-rho}
# Compute reliability under each generating distribution
actual_rho <- numeric(length(gen_shapes))
names(actual_rho) <- gen_shapes

for (j in seq_along(gen_shapes)) {
  theta_check <- sim_latentG(n = 5000, shape = gen_shapes[j], seed = 42)$theta
  actual_rho[j] <- compute_rho_tilde(
    c         = cs2_calib$c_star,
    theta_vec = theta_check,
    beta_vec  = cs2_calib$beta_vec,
    lambda_base = cs2_calib$lambda_base,
    theta_var = var(theta_check)
  )
}

rho_table <- data.frame(
  gen_shape    = gen_shapes,
  target_rho   = 0.80,
  actual_rho   = round(actual_rho, 4),
  rho_shift    = round(actual_rho - 0.80, 4)
)
cat("Reliability under each generating distribution:\n")
print(rho_table)
```

The reliability shifts when the generating distribution differs from the
calibration distribution. This shift arises because the test information
function $\mathcal{J}(\theta; c^*)$ is not uniform across $\theta$, so
different latent distributions weight the information differently.

## Step 3: Run replications

```{r cs2-replications}
cs2_results <- data.frame(
  gen_shape = character(), rep = integer(),
  bias_beta = numeric(), rmse_beta = numeric(),
  mean_p = numeric(),
  stringsAsFactors = FALSE
)

for (j in seq_along(gen_shapes)) {
  for (r in seq_len(R)) {
    sim <- simulate_response_data(
      result       = cs2_calib,
      n_persons    = n_persons,
      latent_shape = gen_shapes[j],
      seed         = 2000 * j + r
    )

    beta_true <- sim$beta
    p_items   <- colMeans(sim$response_matrix)
    p_items   <- pmin(pmax(p_items, 0.001), 0.999)
    beta_hat  <- -log(p_items / (1 - p_items))
    beta_hat  <- beta_hat - mean(beta_hat)

    bias <- mean(beta_hat - beta_true)
    rmse <- sqrt(mean((beta_hat - beta_true)^2))

    cs2_results <- rbind(cs2_results, data.frame(
      gen_shape = gen_shapes[j], rep = r,
      bias_beta = bias, rmse_beta = rmse,
      mean_p = mean(colMeans(sim$response_matrix)),
      stringsAsFactors = FALSE
    ))
  }
}
```

## Results

```{r cs2-summary}
cs2_summary <- aggregate(
  cbind(bias_beta, rmse_beta, mean_p) ~ gen_shape,
  data = cs2_results,
  FUN = function(x) round(mean(x), 4)
)
names(cs2_summary)[2:4] <- c("mean_bias", "mean_RMSE", "mean_prop_correct")

# Add actual reliability
cs2_summary$actual_rho <- round(actual_rho[cs2_summary$gen_shape], 4)
cs2_summary$rho_shift  <- round(actual_rho[cs2_summary$gen_shape] - 0.80, 4)

# Reorder columns for clarity
cs2_summary <- cs2_summary[, c("gen_shape", "actual_rho", "rho_shift",
                                 "mean_bias", "mean_RMSE", "mean_prop_correct")]

cat("Case Study 2: Latent Distribution Robustness Results\n")
cat("====================================================\n")
print(cs2_summary)
```

```{r cs2-plot, fig.height = 6, fig.cap = "Impact of latent distribution misspecification. Top: actual reliability shifts away from the 0.80 target under non-normal generation. Bottom: difficulty RMSE varies with generating shape, driven partly by the reliability shift."}
oldpar <- par(mfrow = c(2, 1), mar = c(4, 4, 3, 1))

# Panel 1: Reliability shift
bar_colors <- c("steelblue", "coral", "seagreen", "orchid")
bp <- barplot(actual_rho, col = bar_colors,
              ylim = c(0.5, 1.0), ylab = "Achieved Reliability",
              main = "Actual Reliability Under Each Generating Distribution",
              names.arg = gen_shapes, las = 1, cex.names = 0.8)
abline(h = 0.80, col = "red", lty = 2, lwd = 2)
text(bp[1], 0.82, "Target = 0.80", col = "red", adj = 0, cex = 0.8)

# Add value labels
text(bp, actual_rho + 0.02, labels = round(actual_rho, 3), cex = 0.8)

# Panel 2: RMSE
rmse_vals <- cs2_summary$mean_RMSE
names(rmse_vals) <- cs2_summary$gen_shape
bp2 <- barplot(rmse_vals, col = bar_colors,
               ylab = "Mean RMSE (Difficulty)",
               main = "Difficulty RMSE by Generating Distribution",
               las = 1, cex.names = 0.8)

# Add value labels
text(bp2, rmse_vals + max(rmse_vals) * 0.03,
     labels = round(rmse_vals, 3), cex = 0.8)

par(oldpar)
```

## Interpretation

When the generating distribution does not match the calibration
distribution, two things happen:

1. **Reliability shifts**: the actual reliability deviates from the target
   (sometimes higher, sometimes lower), depending on how the test
   information profile aligns with the new distribution.

2. **Recovery accuracy changes**: RMSE is partly driven by the reliability
   shift and partly by the distribution's effect on the information
   profile at each ability level.

Key findings:

- **Normal** (matched condition): reliability is at the target, and RMSE
  serves as the baseline.

- **Positively skewed**: more examinees are located in the left tail where
  item information may be lower for items centered near $\beta = 0$.

- **Bimodal**: two modes create regions of low density near $\theta = 0$
  where most items provide maximal information, potentially reducing
  effective information utilization.

- **Heavy-tailed**: extreme ability values fall in regions of very low
  item information, reducing the harmonic mean of information.

This demonstrates why it is critical to either (a) calibrate under the
intended generating distribution, or (b) report the actual achieved
reliability under the generating distribution when intentionally
mismatching.

## Methods text

> To assess robustness to latent distribution misspecification, we
> calibrated item parameters to $\tilde{\rho}^* = 0.80$ under a normal
> latent distribution using EQC ($M = 10{,}000$; $I = 20$ Rasch items).
> Response data were then generated under four distributions: normal,
> positively skewed (Gamma, $k = 4$), bimodal ($\delta = 0.8$), and
> heavy-tailed (Student-t, $df = 5$), all pre-standardized to mean 0 and
> variance 1. For each generating condition, $R = 500$ replications of
> $N = 1{,}000$ simulees were generated. We report both the theoretical
> reliability under each generating distribution and the difficulty recovery
> RMSE.


# Case Study 3: Sample Size Planning

## Research question

> What minimum sample size is needed for stable item difficulty recovery
> at $\tilde{\rho} = 0.80$?

This study varies sample size while holding all other factors constant,
producing an RMSE-vs-N curve that identifies the point of diminishing
returns. This is a fundamental practical question: how many examinees
are needed to achieve adequate parameter recovery?

## Design

| Factor       | Specification                        |
|:-------------|:-------------------------------------|
| Sample size  | 50, 100, 200, 500                    |
| Model        | Rasch                                |
| Reliability  | 0.80                                 |
| Test length  | 20 items                             |
| Replications | 5 (use 500+ in practice)             |

## Step 1: Calibrate once

Since reliability and all other factors are held constant, we calibrate
only once. The same calibrated parameters are used for all sample sizes.

```{r cs3-calibrate}
cs3_calib <- eqc_calibrate(
  target_rho   = 0.80,
  n_items      = 20,
  model        = "rasch",
  item_source  = "parametric",
  M            = 5000L,
  seed         = 42,
  verbose      = FALSE
)

cat(sprintf("Calibrated c* = %.4f, achieved rho = %.4f\n",
            cs3_calib$c_star, cs3_calib$achieved_rho))

sample_sizes <- c(50, 100, 200, 500)
R <- 5
```

## Step 2: Run replications

```{r cs3-replications}
cs3_results <- data.frame(
  n_persons = integer(), rep = integer(),
  rmse_beta = numeric(), bias_beta = numeric(),
  max_abs_err = numeric(),
  stringsAsFactors = FALSE
)

for (n_idx in seq_along(sample_sizes)) {
  N <- sample_sizes[n_idx]

  for (r in seq_len(R)) {
    sim <- simulate_response_data(
      result    = cs3_calib,
      n_persons = N,
      seed      = 3000 * n_idx + r
    )

    beta_true <- sim$beta
    p_items   <- colMeans(sim$response_matrix)
    p_items   <- pmin(pmax(p_items, 0.001), 0.999)
    beta_hat  <- -log(p_items / (1 - p_items))
    beta_hat  <- beta_hat - mean(beta_hat)

    bias     <- mean(beta_hat - beta_true)
    rmse     <- sqrt(mean((beta_hat - beta_true)^2))
    max_abs  <- max(abs(beta_hat - beta_true))

    cs3_results <- rbind(cs3_results, data.frame(
      n_persons = N, rep = r,
      rmse_beta = rmse, bias_beta = bias,
      max_abs_err = max_abs,
      stringsAsFactors = FALSE
    ))
  }
}
```

## Results

```{r cs3-summary}
cs3_summary <- aggregate(
  cbind(bias_beta, rmse_beta, max_abs_err) ~ n_persons,
  data = cs3_results,
  FUN = function(x) round(mean(x), 4)
)
names(cs3_summary)[2:4] <- c("mean_bias", "mean_RMSE", "mean_max_err")

# Add SD of RMSE across reps
cs3_rmse_sd <- aggregate(rmse_beta ~ n_persons, data = cs3_results, FUN = sd)
cs3_summary$sd_RMSE <- round(cs3_rmse_sd$rmse_beta, 4)

cat("Case Study 3: Sample Size Planning Results\n")
cat("============================================\n")
print(cs3_summary)
```

```{r cs3-plot, fig.cap = "Difficulty RMSE as a function of sample size at fixed reliability (rho = 0.80). RMSE decreases monotonically with N. The shaded region shows plus/minus one standard deviation across replications."}
# Prepare data
N_vals <- cs3_summary$n_persons
rmse_mean <- cs3_summary$mean_RMSE
rmse_sd   <- cs3_summary$sd_RMSE

# Plot
plot(N_vals, rmse_mean,
     type = "b", pch = 19, col = "steelblue", lwd = 2,
     xlab = "Sample Size (N)", ylab = "Mean RMSE (Difficulty)",
     main = "Case Study 3: RMSE vs. Sample Size at Fixed Reliability",
     log = "x", ylim = c(0, max(rmse_mean + rmse_sd) * 1.2))

# Shaded confidence region
polygon(
  c(N_vals, rev(N_vals)),
  c(rmse_mean - rmse_sd, rev(rmse_mean + rmse_sd)),
  col = adjustcolor("steelblue", alpha.f = 0.2), border = NA
)

# Re-draw line on top
lines(N_vals, rmse_mean, type = "b", pch = 19, col = "steelblue", lwd = 2)

# Add reference line at N = 200
abline(v = 200, col = "red", lty = 3)
text(200, max(rmse_mean) * 0.95, "N = 200", col = "red", pos = 4, cex = 0.8)

# Add value labels
text(N_vals, rmse_mean + rmse_sd + max(rmse_mean) * 0.05,
     labels = round(rmse_mean, 3), cex = 0.8)

grid(lty = 2, col = "gray80")
```

## RMSE scaling analysis

Theoretical prediction: under regularity conditions, RMSE should scale
approximately as $O(1/\sqrt{N})$. We can check this by examining the
ratio of RMSE values:

```{r cs3-scaling}
# Check the O(1/sqrt(N)) scaling
cat("Theoretical RMSE scaling check:\n")
cat("If RMSE ~ 1/sqrt(N), then RMSE * sqrt(N) should be constant.\n\n")

cs3_summary$rmse_x_sqrtN <- round(cs3_summary$mean_RMSE * sqrt(cs3_summary$n_persons), 4)
print(cs3_summary[, c("n_persons", "mean_RMSE", "rmse_x_sqrtN")])
```

If the product RMSE $\times \sqrt{N}$ is approximately constant across
sample sizes, it confirms the expected $O(1/\sqrt{N})$ scaling.

## Practical recommendation

The RMSE decreases with sample size but exhibits diminishing returns.
Based on this analysis:

- **N < 100**: RMSE is high and variable; difficulty estimates are
  unreliable for individual items. The maximum absolute error may exceed
  0.50 logits.

- **N = 200**: a reasonable minimum for stable difficulty recovery
  at $\tilde{\rho} = 0.80$ with 20 Rasch items. RMSE stabilizes at
  a tolerable level for most applications.

- **N >= 500**: marginal improvement is small relative to the increased
  data collection cost. Recommended only for high-precision applications.

For studies that require difficulty recovery within a specified tolerance
(e.g., RMSE < 0.20), the researcher can read the required N directly
from the RMSE curve. Different reliability levels will shift the curve
vertically: lower reliability requires larger N for the same RMSE.

## Methods text

> To determine the minimum sample size for stable item difficulty recovery,
> we calibrated a 20-item Rasch test to $\tilde{\rho}^* = 0.80$ using EQC.
> Response data were generated for $N \in \{50, 100, 200, 500, 1000\}$
> simulees across $R = 500$ replications per condition. Item difficulties
> were estimated via maximum marginal likelihood and evaluated by RMSE.
> Diminishing returns in RMSE were observed beyond $N = 200$, which we
> recommend as the minimum sample size for this configuration.


# Adapt for Your Study

The three case studies above are templates. To adapt them for your
research, follow the planning and reporting checklists below.

## Planning checklist

Before running your study, verify the following:

1. **Define the research question** clearly and precisely. The question
   should specify what comparison is being made and what metrics will
   be used to evaluate it.

2. **Select the IRT model(s)** appropriate for your context. Use `"rasch"`
   for tests with equal discriminations and `"2pl"` for tests with varying
   discriminations.

3. **Choose the latent distribution(s)** that represent your target
   population. Use `"normal"` as a baseline and add non-normal shapes
   to test robustness.

4. **Set target reliability levels** that span the range relevant to your
   application. Include at least three levels to detect nonlinear effects.

5. **Screen all design cells** for feasibility using `check_feasibility()`.
   Remove or modify any infeasible cells.

6. **Determine the number of replications.** For published work, $R \geq 500$
   is standard. For pilot studies, $R = 100$ may suffice.

7. **Establish a seed management scheme** for complete reproducibility.
   Record all seeds in your analysis script.

## Template code

The following template can be adapted for any simulation study using
IRTsimrel. Replace the bracketed comments with your study's parameters.

```{r template, eval = FALSE}
# =============================================================
# IRTsimrel Simulation Study Template
# =============================================================
#
# Research Question:
#   [State your research question here]
#
# Author: [Your name]
# Date:   [Date]
# =============================================================

library(IRTsimrel)

# ---- 1. Design ----
design <- expand.grid(
  target_rho   = c(0.60, 0.70, 0.80),      # [your reliability levels]
  n_items      = c(20, 40),                  # [your test lengths]
  model        = c("rasch", "2pl"),          # [your models]
  latent_shape = c("normal", "skew_pos"),    # [your latent shapes]
  stringsAsFactors = FALSE
)

N_PERSONS  <- 1000     # [your sample size]
R          <- 500      # [your replications]
M_QUAD     <- 10000L   # [your quadrature size]
CALIB_SEED <- 42       # [your calibration seed]

# ---- 2. Feasibility screening ----
design$feasible <- NA
for (i in seq_len(nrow(design))) {
  feas <- check_feasibility(
    n_items      = design$n_items[i],
    model        = design$model[i],
    latent_shape = design$latent_shape[i],
    item_source  = "parametric",
    M            = M_QUAD,
    seed         = CALIB_SEED,
    verbose      = FALSE
  )
  design$feasible[i] <- design$target_rho[i] >= feas$rho_range_info[1] &
                         design$target_rho[i] <= feas$rho_range_info[2]
}
design <- design[design$feasible, ]
cat(sprintf("Feasible cells: %d\n", nrow(design)))

# ---- 3. Calibration ----
calib_list <- vector("list", nrow(design))
for (i in seq_len(nrow(design))) {
  d <- design[i, ]
  calib_list[[i]] <- eqc_calibrate(
    target_rho   = d$target_rho,
    n_items      = d$n_items,
    model        = d$model,
    latent_shape = d$latent_shape,
    item_source  = "parametric",
    M            = M_QUAD,
    seed         = CALIB_SEED,
    verbose      = FALSE
  )
}

# ---- 4. Verification ----
design$achieved_rho <- vapply(calib_list,
  function(r) r$achieved_rho, numeric(1))
design$abs_error <- abs(design$achieved_rho - design$target_rho)
cat(sprintf("Max calibration error: %.2e\n", max(design$abs_error)))

# ---- 5. Simulation loop ----
output <- data.frame()

for (i in seq_len(nrow(design))) {
  d <- design[i, ]

  for (r in seq_len(R)) {
    sim <- simulate_response_data(
      result       = calib_list[[i]],
      n_persons    = N_PERSONS,
      latent_shape = d$latent_shape,
      seed         = 1000 * i + r
    )

    # ---- Your analysis here ----
    beta_true <- sim$beta
    p_items <- colMeans(sim$response_matrix)
    p_items <- pmin(pmax(p_items, 0.001), 0.999)
    beta_hat <- -log(p_items / (1 - p_items))
    beta_hat <- beta_hat - mean(beta_hat)
    rmse <- sqrt(mean((beta_hat - beta_true)^2))
    bias <- mean(beta_hat - beta_true)

    output <- rbind(output, data.frame(
      cell = i, rep = r,
      target_rho = d$target_rho, model = d$model,
      n_items = d$n_items, latent_shape = d$latent_shape,
      rmse = rmse, bias = bias
    ))
  }
}

# ---- 6. Summarize ----
summary_table <- aggregate(
  cbind(rmse, bias) ~ target_rho + model + n_items + latent_shape,
  data = output, FUN = mean
)
print(summary_table)
```

## Reporting checklist

When writing up your simulation study, include the following:

1. State IRTsimrel version and cite Lee (2025).
2. Specify the calibration algorithm (EQC or SAC) and its parameters.
3. Report the reliability metric (average-information or MSEM-based).
4. List all target reliability levels.
5. Describe item parameter source and generation method.
6. State latent distribution(s) and shape parameters.
7. Report feasibility screening results (number screened, number infeasible).
8. Report maximum calibration error across all cells.
9. State the number of replications and persons per replication.
10. Describe seed management for reproducibility.


# Summary and Recommendations

The three case studies illustrate the following general principles:

**Principle 1: Always control reliability.** Without it, model comparisons
are confounded by data quality differences. Case Study 1 demonstrates how
reliability control isolates the effect of model structure on parameter
recovery. The differences within a model across reliability levels are
often larger than the differences between models at a fixed reliability.

**Principle 2: Document distribution mismatches.** If the calibration and
generating distributions differ, report the actual achieved reliability
under the generating distribution. Case Study 2 shows that misspecification
shifts both reliability and recovery accuracy in predictable but important
ways.

**Principle 3: Use RMSE curves for sample size planning.** A
reliability-controlled RMSE-vs-N curve provides an objective basis for
choosing sample size. Case Study 3 demonstrates diminishing returns beyond
a minimum threshold and confirms the theoretical $O(1/\sqrt{N})$ scaling.

**Principle 4: Start with feasibility screening.** Before committing to a
large simulation, use `check_feasibility()` and `rho_curve()` to verify
that your design is achievable. Infeasible cells waste computational
resources and produce misleading boundary solutions.

**Principle 5: Report thoroughly.** Follow the reporting checklist and
include a methods paragraph based on the templates provided. Transparent
reporting enables replication and builds confidence in your findings.

For additional background, see:

- `vignette("introduction")` -- package overview.
- `vignette("simulation-design")` -- factorial design framework.
- `vignette("algorithm-eqc")` -- Algorithm 1 details.
- `vignette("algorithm-sac")` -- Algorithm 2 (SAC) for validation.
- `vignette("validation")` -- TAM-based validation procedures.


# References

Lee, J. (2025). Reliability-targeted simulation of item response data:
Solving the inverse design problem. *arXiv preprint arXiv:2512.16012*.

Robbins, H., & Monro, S. (1951). A stochastic approximation method.
*The Annals of Mathematical Statistics, 22*(3), 400--407.
